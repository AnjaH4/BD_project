{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24771/3836109259.py:8: DtypeWarning: Columns (17,20,22,23,30,31,32,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(csv_file_path, chunksize=chunksize):\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] Cannot create directory 'data/parking_violations_fy2024.parquet': non-directory entry exists. Detail: [errno 17] File exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m         first_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;66;03m# Append subsequent chunks with write_to_dataset\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m         \u001b[43mpq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_to_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCSV to Parquet conversion completed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bd_project/lib/python3.9/site-packages/pyarrow/parquet/core.py:3422\u001b[0m, in \u001b[0;36mwrite_to_dataset\u001b[0;34m(table, root_path, partition_cols, partition_filename_cb, filesystem, use_legacy_dataset, schema, partitioning, basename_template, use_threads, file_visitor, existing_data_behavior, **kwargs)\u001b[0m\n\u001b[1;32m   3419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m existing_data_behavior \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3420\u001b[0m         existing_data_behavior \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverwrite_or_ignore\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 3422\u001b[0m     \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3424\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_visitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_visitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbasename_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbasename_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexisting_data_behavior\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexisting_data_behavior\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3429\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mwrite_dataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   3432\u001b[0m \u001b[38;5;66;03m# warnings and errors when using legacy implementation\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bd_project/lib/python3.9/site-packages/pyarrow/dataset.py:1018\u001b[0m, in \u001b[0;36mwrite_dataset\u001b[0;34m(data, base_dir, basename_template, format, partitioning, partitioning_flavor, schema, filesystem, file_options, use_threads, max_partitions, max_open_files, max_rows_per_file, min_rows_per_group, max_rows_per_group, file_visitor, existing_data_behavior, create_dir)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot specify a schema when writing a Scanner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1016\u001b[0m     scanner \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m-> 1018\u001b[0m \u001b[43m_filesystemdataset_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscanner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasename_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_partitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_visitor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexisting_data_behavior\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_open_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows_per_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_rows_per_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows_per_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_dir\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bd_project/lib/python3.9/site-packages/pyarrow/_dataset.pyx:3919\u001b[0m, in \u001b[0;36mpyarrow._dataset._filesystemdataset_write\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/bd_project/lib/python3.9/site-packages/pyarrow/error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] Cannot create directory 'data/parking_violations_fy2024.parquet': non-directory entry exists. Detail: [errno 17] File exists"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "csv_file_path = 'data/Parking_Violations_Issued_-_Fiscal_Year_2024_20240524.csv'\n",
    "parquet_file_path = 'data/parking_violations_fy2024.parquet'\n",
    "chunksize = 100000  # Number of rows per chunk\n",
    "\n",
    "# Initialize an empty Parquet file\n",
    "first_chunk = True\n",
    "\n",
    "for chunk in pd.read_csv(csv_file_path, chunksize=chunksize):\n",
    "    table = pa.Table.from_pandas(chunk)\n",
    "    if first_chunk:\n",
    "        # Write the first chunk with write_table\n",
    "        pq.write_table(table, parquet_file_path)\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        # Append subsequent chunks with write_to_dataset\n",
    "        pq.write_to_dataset(table, root_path=parquet_file_path)\n",
    "\n",
    "print(\"CSV to Parquet conversion completed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
